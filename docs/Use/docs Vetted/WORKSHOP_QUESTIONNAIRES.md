# Workshop Questionnaires
**Pre-Workshop & Post-Workshop Surveys**

---

## ğŸ“‹ PRE-WORKSHOP QUESTIONNAIRE (5 minutes)

**Instructions**: Please complete this brief survey before the workshop begins. Your responses are anonymous and will be used to improve the workshop and for research purposes.

---

### **Participant ID**: __________ (Assigned by facilitator)

---

### **Section 1: Role Selection** (Required)

**Q1. Which role will you take in today's workshop?**

- â˜ **Orchestrator** (Non-technical: I will design workflows visually using AI Studio)
- â˜ **Implementer** (Technical: I will write code to implement the agent)
- â˜ **Undecided** (I'll choose during the workshop)

---

### **Section 2: Background & Experience**

**Q2. What is your experience with AI/Machine Learning?**

- â˜ None (Never worked with AI)
- â˜ Beginner (Familiar with concepts, minimal hands-on)
- â˜ Intermediate (Built simple AI projects)
- â˜ Advanced (Built production AI systems)

**Q3. What is your programming experience?**

- â˜ None (No coding experience)
- â˜ Beginner (Basic scripting)
- â˜ Intermediate (Can build applications)
- â˜ Advanced (Professional developer)

**Q4. Have you used any of these before?** (Check all that apply)

- â˜ LangChain or LangGraph
- â˜ RAG (Retrieval Augmented Generation)
- â˜ Vector databases (Chroma, Pinecone, etc.)
- â˜ Google AI Studio or Gemini API
- â˜ None of the above

**Q5. What is your primary occupation?**

- â˜ Software Developer/Engineer
- â˜ Data Scientist/ML Engineer
- â˜ Product Manager
- â˜ Designer/UX
- â˜ Business Analyst
- â˜ Student
- â˜ Other: __________

---

### **Section 3: Goals & Expectations**

**Q6. What do you hope to learn from this workshop?** (Check top 2)

- â˜ How to build Agentic AI systems
- â˜ How to use LangGraph
- â˜ How to implement RAG
- â˜ How to collaborate on AI projects
- â˜ How to deploy AI to production
- â˜ Other: __________

**Q7. How confident are you in the following?** (Rate 1-5 for each)

**If you plan to be an Orchestrator:**
- Designing agent workflows visually: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Creating effective test cases: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Writing knowledge base content: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Collaborating with developers: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5

**If you plan to be an Implementer:**
- Building LangGraph workflows: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Implementing RAG systems: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Deploying to production: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Collaborating with non-technical stakeholders: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5

---

### **Section 4: Consent** (Required)

**Q8. I consent to:**

- â˜ Participate in this workshop
- â˜ My anonymous data being used for research/publication
- â˜ Being photographed/recorded (optional)

**Signature**: _________________ **Date**: _______

---

**Thank you! Please return this form to the facilitator.**

---
---

## ğŸ“Š POST-WORKSHOP QUESTIONNAIRE (5 minutes)

**Instructions**: Please complete this survey after the workshop. Your honest feedback helps us improve!

---

### **Participant ID**: __________ (Same as pre-workshop)

---

### **Section 1: Completion & Success**

**Q1. What role did you take?**

- â˜ Orchestrator (Designed workflows visually)
- â˜ Implementer (Wrote code)
- â˜ Both (Switched between roles)

**Q2. Did you successfully complete the following?** (Check all that apply)

- â˜ Created system prompt / personality
- â˜ Designed visual workflow (Orchestrators) OR Built LangGraph agent (Implementers)
- â˜ Created test cases (Orchestrators) OR Implemented RAG system (Implementers)
- â˜ Tested the agent locally
- â˜ Deployed agent to Cloud Run
- â˜ None of the above

**Q3. How long did it take you to get a working agent?**

- â˜ Less than 30 minutes
- â˜ 30-45 minutes
- â˜ 45-60 minutes
- â˜ 60-75 minutes
- â˜ 75-90 minutes
- â˜ Did not complete

**Q4. Did your agent pass the test cases?**

- â˜ All 5 test cases passed
- â˜ 4 test cases passed
- â˜ 3 test cases passed
- â˜ 2 test cases passed
- â˜ 1 test case passed
- â˜ 0 test cases passed / Did not test

---

### **Section 2: Learning & Confidence (Before/After Comparison)**

**Q5. How confident are you NOW in the following?** (Rate 1-5 for each)

**If you were an Orchestrator:**
- Designing agent workflows visually: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Creating effective test cases: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Writing knowledge base content: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Collaborating with developers: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5

**If you were an Implementer:**
- Building LangGraph workflows: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Implementing RAG systems: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Deploying to production: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- Collaborating with non-technical stakeholders: â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5

**Q6. I now understand the 6 Agentic AI principles:**

- â˜ 1 - Strongly Disagree
- â˜ 2 - Disagree
- â˜ 3 - Neutral
- â˜ 4 - Agree
- â˜ 5 - Strongly Agree

**Q7. I feel confident I could build a simple AI agent on my own:**

- â˜ 1 - Strongly Disagree
- â˜ 2 - Disagree
- â˜ 3 - Neutral
- â˜ 4 - Agree
- â˜ 5 - Strongly Agree

---

### **Section 3: Collaboration Experience**

**Q8. How happy were you working with your partner(s)?**

- â˜ 1 - Very Unhappy
- â˜ 2 - Unhappy
- â˜ 3 - Neutral
- â˜ 4 - Happy
- â˜ 5 - Very Happy

**Q9. Collaboration between Orchestrators and Implementers was effective:**

- â˜ 1 - Strongly Disagree
- â˜ 2 - Disagree
- â˜ 3 - Neutral
- â˜ 4 - Agree
- â˜ 5 - Strongly Agree

**Q10. Rate your experience with the following:** (1-5 scale)

**For Orchestrators:**
- How well did implementers understand your designs? â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- How quickly did they implement your ideas? â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- How open were they to your feedback? â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5

**For Implementers:**
- How clear were the orchestrator's designs? â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- How realistic were their requirements? â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5
- How helpful was their testing feedback? â˜ 1  â˜ 2  â˜ 3  â˜ 4  â˜ 5

**Q11. How many design decisions did you contribute?** (Estimate)

- â˜ 0-2 decisions
- â˜ 3-5 decisions
- â˜ 6-10 decisions
- â˜ 10+ decisions

**Q12. Would you prefer to work with this collaboration model in the future?**

- â˜ Yes, definitely
- â˜ Yes, probably
- â˜ Not sure
- â˜ Probably not
- â˜ Definitely not

---

### **Section 4: Overall Satisfaction**

**Q13. Overall, how would you rate this workshop?**

- â˜ 1 - Poor
- â˜ 2 - Fair
- â˜ 3 - Good
- â˜ 4 - Very Good
- â˜ 5 - Excellent

**Q14. Would you recommend this workshop to a colleague?**

- â˜ Yes
- â˜ No
- â˜ Maybe

**Q15. What was the MOST valuable part of the workshop?** (1-2 sentences)

_________________________________________________________________

_________________________________________________________________

**Q16. What changes would you recommend?** (Be specific to your role)

**For Orchestrators:**
- What would make visual design easier? _________________________________
- What additional tools would help? ____________________________________
- What was confusing about the process? ________________________________

**For Implementers:**
- What would make implementation easier? _______________________________
- What code/documentation was missing? _________________________________
- What technical challenges did you face? ______________________________

**Q17. Any other feedback or suggestions?**

_________________________________________________________________

_________________________________________________________________

---

**Thank you for your feedback!**

---
---

## ğŸ“ˆ DATA COLLECTION TRACKING SHEET (For Facilitator)

**Workshop Date**: ___________  
**Total Participants**: ___________

---

### **Real-Time Metrics** (Track during workshop)

| Participant ID | Role (O/I) | Completed Setup | Working Agent | Deployed | Test Cases Passed | Time to Working Agent |
|----------------|------------|-----------------|---------------|----------|-------------------|-----------------------|
| P001 | | â˜ | â˜ | â˜ | ___/5 | ___ min |
| P002 | | â˜ | â˜ | â˜ | ___/5 | ___ min |
| P003 | | â˜ | â˜ | â˜ | ___/5 | ___ min |
| ... | | | | | | |

**Legend**: O = Orchestrator, I = Implementer

---

### **Workshop Milestones** (Record times)

- **Workshop Start**: _____
- **First working agent**: _____ (Participant ID: ____)
- **First deployment**: _____ (Participant ID: ____)
- **Workshop End**: _____

---

### **Issues Encountered** (For analysis)

| Issue | Count | Resolution |
|-------|-------|------------|
| API key problems | ___ | |
| Installation errors | ___ | |
| LangGraph errors | ___ | |
| RAG initialization | ___ | |
| Deployment failures | ___ | |
| Other: _________ | ___ | |

---

### **Observations** (Qualitative notes)

**Orchestrator Contributions**:
_________________________________________________________________

**Implementer Challenges**:
_________________________________________________________________

**Collaboration Highlights**:
_________________________________________________________________

**Unexpected Outcomes**:
_________________________________________________________________

---
---

## ğŸ“Š METRICS FOR IEEE PAPER

### **From Pre-Workshop Survey**:

1. **Participant Demographics**
   - Total N
   - Orchestrators vs Implementers (%)
   - AI experience distribution
   - Programming experience distribution
   - Prior tool usage (%)

2. **Baseline Confidence**
   - Average confidence score (Q7)
   - Distribution by role

---

### **From Post-Workshop Survey**:

1. **Success Metrics**
   - Completion rate (% who completed each step)
   - Deployment success rate (%)
   - Test case pass rate (average)
   - Time to working agent (average, median, SD)

2. **Learning Outcomes**
   - Understanding of Agentic AI (average score Q5)
   - Confidence to build independently (average score Q6)
   - Pre/post confidence change

3. **Tool Effectiveness**
   - Visual tools rating (Orchestrators, Q7)
   - Code quality rating (Implementers, Q8)

4. **Collaboration**
   - Collaboration effectiveness (average Q9)
   - Design contribution distribution (Q10)

5. **Satisfaction**
   - Overall rating (average Q11)
   - Recommendation rate (% Yes on Q12)

---

### **From Facilitator Tracking**:

1. **Time Efficiency**
   - Average time to first working agent
   - Fastest completion time
   - Slowest completion time
   - Deployment success rate

2. **Issues & Resolution**
   - Common problems encountered
   - Resolution strategies
   - Support required

---

### **Calculated Metrics for Paper**:

```
Success Rate = (Participants with working agent / Total) Ã— 100%

Average Completion Time = Sum(completion times) / N

Test Case Pass Rate = Average(test cases passed / 5) Ã— 100%

Deployment Success = (Deployed agents / Total) Ã— 100%

Confidence Improvement = Post-confidence - Pre-confidence

Recommendation Rate = (Yes responses / Total) Ã— 100%

Orchestrator Contribution = (Orchestrator decisions / Total decisions) Ã— 100%
```

---

## ğŸ¯ QUICK SETUP GUIDE

### **Before Workshop**:

1. **Print Forms**:
   - Pre-workshop questionnaire (40 copies)
   - Post-workshop questionnaire (40 copies)
   - Consent forms (40 copies)

2. **Prepare Tracking**:
   - Facilitator tracking sheet (1 copy)
   - Participant ID labels (P001-P040)
   - Clipboards for surveys

3. **Setup Collection**:
   - Box for completed pre-surveys
   - Box for completed post-surveys
   - Pens/pencils

---

### **During Workshop**:

**0:00-0:05** | Distribute pre-survey
- Hand out as participants arrive
- Assign participant IDs
- Collect before starting

**0:05-1:20** | Track metrics
- Note completion milestones
- Record issues
- Observe collaboration

**1:20-1:25** | Distribute post-survey
- Hand out to all participants
- Give 5 minutes to complete
- Collect before leaving

---

### **After Workshop**:

**Day 1**: 
- Enter data into spreadsheet
- Calculate basic metrics
- Scan/backup all forms

**Week 1**:
- Analyze data
- Create charts/tables
- Identify key findings

**Week 2**:
- Write results section
- Include in IEEE paper

---

**All forms ready to print and use!** ğŸ“‹
